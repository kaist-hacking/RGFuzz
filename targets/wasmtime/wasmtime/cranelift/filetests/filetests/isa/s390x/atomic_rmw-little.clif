test compile precise-output
target s390x

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; ATOMIC_RMW (XCHG)
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %atomic_rmw_xchg_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little xchg v1, v2
  return v3
}

; VCode:
; block0:
;   lrvgr %r2, %r4
;   lg %r0, 0(%r3)
;   0: csg %r0, %r2, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvgr %r2, %r4
;   lg %r0, 0(%r3)
;   csg %r0, %r2, 0(%r3)
;   jglh 0xa
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_xchg_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little xchg v1, v2
  return v3
}

; VCode:
; block0:
;   lrvr %r2, %r4
;   l %r0, 0(%r3)
;   0: cs %r0, %r2, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvr %r2, %r4
;   l %r0, 0(%r3)
;   cs %r0, %r2, 0(%r3)
;   jglh 8
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_xchg_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little xchg v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; risbgn %r1, %r4, 48, 64, 48 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   risbgn %r1, %r4, 0x30, 0x40, 0x30
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x12
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_xchg_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little xchg v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; risbgn %r1, %r4, 32, 40, 24 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   risbgn %r1, %r4, 0x20, 0x28, 0x18
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x10
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_add_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little add v1, v2
  return v3
}

; VCode:
; block0:
;   lg %r0, 0(%r3)
;   0: lrvgr %r1, %r0 ; agr %r1, %r4 ; lrvgr %r1, %r1 ; csg %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lg %r0, 0(%r3)
;   lrvgr %r1, %r0
;   agr %r1, %r4
;   lrvgr %r1, %r1
;   csg %r0, %r1, 0(%r3)
;   jglh 6
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_add_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little add v1, v2
  return v3
}

; VCode:
; block0:
;   l %r0, 0(%r3)
;   0: lrvr %r1, %r0 ; ar %r1, %r4 ; lrvr %r1, %r1 ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   l %r0, 0(%r3)
;   lrvr %r1, %r0
;   ar %r1, %r4
;   lrvr %r1, %r1
;   cs %r0, %r1, 0(%r3)
;   jglh 4
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_add_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little add v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 16
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; lrvr %r1, %r1 ; ar %r1, %r4 ; lrvr %r1, %r1 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x10
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   lrvr %r1, %r1
;   ar %r1, %r4
;   lrvr %r1, %r1
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x14
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_add_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little add v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 24
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; ar %r1, %r4 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x18
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   ar %r1, %r4
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x16
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_sub_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little sub v1, v2
  return v3
}

; VCode:
; block0:
;   lg %r0, 0(%r3)
;   0: lrvgr %r1, %r0 ; sgr %r1, %r4 ; lrvgr %r1, %r1 ; csg %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lg %r0, 0(%r3)
;   lrvgr %r1, %r0
;   sgr %r1, %r4
;   lrvgr %r1, %r1
;   csg %r0, %r1, 0(%r3)
;   jglh 6
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_sub_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little sub v1, v2
  return v3
}

; VCode:
; block0:
;   l %r0, 0(%r3)
;   0: lrvr %r1, %r0 ; sr %r1, %r4 ; lrvr %r1, %r1 ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   l %r0, 0(%r3)
;   lrvr %r1, %r0
;   sr %r1, %r4
;   lrvr %r1, %r1
;   cs %r0, %r1, 0(%r3)
;   jglh 4
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_sub_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little sub v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 16
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; lrvr %r1, %r1 ; sr %r1, %r4 ; lrvr %r1, %r1 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x10
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   lrvr %r1, %r1
;   sr %r1, %r4
;   lrvr %r1, %r1
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x14
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_sub_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little sub v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 24
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; sr %r1, %r4 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x18
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   sr %r1, %r4
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x16
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_and_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little and v1, v2
  return v3
}

; VCode:
; block0:
;   lrvgr %r2, %r4
;   lang %r4, %r2, 0(%r3)
;   lrvgr %r2, %r4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvgr %r2, %r4
;   lang %r4, %r2, 0(%r3)
;   lrvgr %r2, %r4
;   br %r14

function %atomic_rmw_and_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little and v1, v2
  return v3
}

; VCode:
; block0:
;   lrvr %r2, %r4
;   lan %r4, %r2, 0(%r3)
;   lrvr %r2, %r4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvr %r2, %r4
;   lan %r4, %r2, 0(%r3)
;   lrvr %r2, %r4
;   br %r14

function %atomic_rmw_and_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little and v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; rnsbg %r1, %r4, 48, 64, 48 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   rnsbg %r1, %r4, 0x30, 0x40, 0x30
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x12
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_and_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little and v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; rnsbg %r1, %r4, 32, 40, 24 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   rnsbg %r1, %r4, 0x20, 0x28, 0x18
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x10
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_or_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little or v1, v2
  return v3
}

; VCode:
; block0:
;   lrvgr %r2, %r4
;   laog %r4, %r2, 0(%r3)
;   lrvgr %r2, %r4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvgr %r2, %r4
;   laog %r4, %r2, 0(%r3)
;   lrvgr %r2, %r4
;   br %r14

function %atomic_rmw_or_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little or v1, v2
  return v3
}

; VCode:
; block0:
;   lrvr %r2, %r4
;   lao %r4, %r2, 0(%r3)
;   lrvr %r2, %r4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvr %r2, %r4
;   lao %r4, %r2, 0(%r3)
;   lrvr %r2, %r4
;   br %r14

function %atomic_rmw_or_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little or v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; rosbg %r1, %r4, 48, 64, 48 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   rosbg %r1, %r4, 0x30, 0x40, 0x30
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x12
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_or_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little or v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; rosbg %r1, %r4, 32, 40, 24 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   rosbg %r1, %r4, 0x20, 0x28, 0x18
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x10
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_xor_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little xor v1, v2
  return v3
}

; VCode:
; block0:
;   lrvgr %r2, %r4
;   laxg %r4, %r2, 0(%r3)
;   lrvgr %r2, %r4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvgr %r2, %r4
;   laxg %r4, %r2, 0(%r3)
;   lrvgr %r2, %r4
;   br %r14

function %atomic_rmw_xor_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little xor v1, v2
  return v3
}

; VCode:
; block0:
;   lrvr %r2, %r4
;   lax %r4, %r2, 0(%r3)
;   lrvr %r2, %r4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvr %r2, %r4
;   lax %r4, %r2, 0(%r3)
;   lrvr %r2, %r4
;   br %r14

function %atomic_rmw_xor_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little xor v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; rxsbg %r1, %r4, 48, 64, 48 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   rxsbg %r1, %r4, 0x30, 0x40, 0x30
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x12
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_xor_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little xor v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; rxsbg %r1, %r4, 32, 40, 24 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   rxsbg %r1, %r4, 0x20, 0x28, 0x18
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x10
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_nand_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little nand v1, v2
  return v3
}

; VCode:
; block0:
;   lrvgr %r2, %r4
;   lg %r0, 0(%r3)
;   0: ngrk %r1, %r0, %r2 ; xilf %r1, 4294967295 ; xihf %r1, 4294967295 ; csg %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvgr %r2, %r4
;   lg %r0, 0(%r3)
;   ngrk %r1, %r0, %r2
;   xilf %r1, 0xffffffff
;   xihf %r1, 0xffffffff
;   csg %r0, %r1, 0(%r3)
;   jglh 0xa
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_nand_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little nand v1, v2
  return v3
}

; VCode:
; block0:
;   lrvr %r2, %r4
;   l %r0, 0(%r3)
;   0: nrk %r1, %r0, %r2 ; xilf %r1, 4294967295 ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvr %r2, %r4
;   l %r0, 0(%r3)
;   nrk %r1, %r0, %r2
;   xilf %r1, 0xffffffff
;   cs %r0, %r1, 0(%r3)
;   jglh 8
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_nand_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little nand v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; rnsbg %r1, %r4, 48, 64, 48 ; xilf %r1, 65535 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lrvr %r4, %r4
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   rnsbg %r1, %r4, 0x30, 0x40, 0x30
;   xilf %r1, 0xffff
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x12
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_nand_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little nand v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; rnsbg %r1, %r4, 32, 40, 24 ; xilf %r1, 4278190080 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   rnsbg %r1, %r4, 0x20, 0x28, 0x18
;   xilf %r1, 0xff000000
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x10
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_smin_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little smin v1, v2
  return v3
}

; VCode:
; block0:
;   lg %r0, 0(%r3)
;   0: lrvgr %r1, %r0 ; cgr %r4, %r1 ; jgnl 1f ; lrvgr %r1, %r4 ; csg %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lg %r0, 0(%r3)
;   lrvgr %r1, %r0
;   cgr %r4, %r1
;   jgnl 0x24
;   lrvgr %r1, %r4
;   csg %r0, %r1, 0(%r3)
;   jglh 6
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_smin_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little smin v1, v2
  return v3
}

; VCode:
; block0:
;   l %r0, 0(%r3)
;   0: lrvr %r1, %r0 ; cr %r4, %r1 ; jgnl 1f ; lrvr %r1, %r4 ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   l %r0, 0(%r3)
;   lrvr %r1, %r0
;   cr %r4, %r1
;   jgnl 0x1e
;   lrvr %r1, %r4
;   cs %r0, %r1, 0(%r3)
;   jglh 4
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_smin_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little smin v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 16
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; lrvr %r1, %r1 ; cr %r4, %r1 ; jgnl 1f ; risbgn %r1, %r4, 32, 48, 0 ; lrvr %r1, %r1 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x10
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   lrvr %r1, %r1
;   cr %r4, %r1
;   jgnl 0x40
;   risbgn %r1, %r4, 0x20, 0x30, 0
;   lrvr %r1, %r1
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x14
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_smin_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little smin v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 24
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; cr %r4, %r1 ; jgnl 1f ; risbgn %r1, %r4, 32, 40, 0 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x18
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   cr %r4, %r1
;   jgnl 0x3a
;   risbgn %r1, %r4, 0x20, 0x28, 0
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x16
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_smax_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little smax v1, v2
  return v3
}

; VCode:
; block0:
;   lg %r0, 0(%r3)
;   0: lrvgr %r1, %r0 ; cgr %r4, %r1 ; jgnh 1f ; lrvgr %r1, %r4 ; csg %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lg %r0, 0(%r3)
;   lrvgr %r1, %r0
;   cgr %r4, %r1
;   jgnh 0x24
;   lrvgr %r1, %r4
;   csg %r0, %r1, 0(%r3)
;   jglh 6
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_smax_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little smax v1, v2
  return v3
}

; VCode:
; block0:
;   l %r0, 0(%r3)
;   0: lrvr %r1, %r0 ; cr %r4, %r1 ; jgnh 1f ; lrvr %r1, %r4 ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   l %r0, 0(%r3)
;   lrvr %r1, %r0
;   cr %r4, %r1
;   jgnh 0x1e
;   lrvr %r1, %r4
;   cs %r0, %r1, 0(%r3)
;   jglh 4
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_smax_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little smax v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 16
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; lrvr %r1, %r1 ; cr %r4, %r1 ; jgnh 1f ; risbgn %r1, %r4, 32, 48, 0 ; lrvr %r1, %r1 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x10
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   lrvr %r1, %r1
;   cr %r4, %r1
;   jgnh 0x40
;   risbgn %r1, %r4, 0x20, 0x30, 0
;   lrvr %r1, %r1
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x14
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_smax_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little smax v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 24
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; cr %r4, %r1 ; jgnh 1f ; risbgn %r1, %r4, 32, 40, 0 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x18
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   cr %r4, %r1
;   jgnh 0x3a
;   risbgn %r1, %r4, 0x20, 0x28, 0
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x16
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_umin_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little umin v1, v2
  return v3
}

; VCode:
; block0:
;   lg %r0, 0(%r3)
;   0: lrvgr %r1, %r0 ; clgr %r4, %r1 ; jgnl 1f ; lrvgr %r1, %r4 ; csg %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lg %r0, 0(%r3)
;   lrvgr %r1, %r0
;   clgr %r4, %r1
;   jgnl 0x24
;   lrvgr %r1, %r4
;   csg %r0, %r1, 0(%r3)
;   jglh 6
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_umin_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little umin v1, v2
  return v3
}

; VCode:
; block0:
;   l %r0, 0(%r3)
;   0: lrvr %r1, %r0 ; clr %r4, %r1 ; jgnl 1f ; lrvr %r1, %r4 ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   l %r0, 0(%r3)
;   lrvr %r1, %r0
;   clr %r4, %r1
;   jgnl 0x1e
;   lrvr %r1, %r4
;   cs %r0, %r1, 0(%r3)
;   jglh 4
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_umin_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little umin v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 16
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; lrvr %r1, %r1 ; clr %r4, %r1 ; jgnl 1f ; risbgn %r1, %r4, 32, 48, 0 ; lrvr %r1, %r1 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x10
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   lrvr %r1, %r1
;   clr %r4, %r1
;   jgnl 0x40
;   risbgn %r1, %r4, 0x20, 0x30, 0
;   lrvr %r1, %r1
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x14
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_umin_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little umin v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 24
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; clr %r4, %r1 ; jgnl 1f ; risbgn %r1, %r4, 32, 40, 0 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x18
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   clr %r4, %r1
;   jgnl 0x3a
;   risbgn %r1, %r4, 0x20, 0x28, 0
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x16
;   rll %r2, %r0, 8(%r2)
;   br %r14

function %atomic_rmw_umax_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = atomic_rmw.i64 little umax v1, v2
  return v3
}

; VCode:
; block0:
;   lg %r0, 0(%r3)
;   0: lrvgr %r1, %r0 ; clgr %r4, %r1 ; jgnh 1f ; lrvgr %r1, %r4 ; csg %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvgr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lg %r0, 0(%r3)
;   lrvgr %r1, %r0
;   clgr %r4, %r1
;   jgnh 0x24
;   lrvgr %r1, %r4
;   csg %r0, %r1, 0(%r3)
;   jglh 6
;   lrvgr %r2, %r0
;   br %r14

function %atomic_rmw_umax_i32(i64, i64, i32) -> i32 {
block0(v0: i64, v1: i64, v2: i32):
  v3 = atomic_rmw.i32 little umax v1, v2
  return v3
}

; VCode:
; block0:
;   l %r0, 0(%r3)
;   0: lrvr %r1, %r0 ; clr %r4, %r1 ; jgnh 1f ; lrvr %r1, %r4 ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   lrvr %r2, %r0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   l %r0, 0(%r3)
;   lrvr %r1, %r0
;   clr %r4, %r1
;   jgnh 0x1e
;   lrvr %r1, %r4
;   cs %r0, %r1, 0(%r3)
;   jglh 4
;   lrvr %r2, %r0
;   br %r14

function %atomic_rmw_umax_i16(i64, i64, i16) -> i16 {
block0(v0: i64, v1: i64, v2: i16):
  v3 = atomic_rmw.i16 little umax v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 16
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 16(%r2) ; lrvr %r1, %r1 ; clr %r4, %r1 ; jgnh 1f ; risbgn %r1, %r4, 32, 48, 0 ; lrvr %r1, %r1 ; rll %r1, %r1, 16(%r2) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x10
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0x10(%r2)
;   lrvr %r1, %r1
;   clr %r4, %r1
;   jgnh 0x40
;   risbgn %r1, %r4, 0x20, 0x30, 0
;   lrvr %r1, %r1
;   rll %r1, %r1, 0x10(%r2)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x14
;   rll %r2, %r0, 0(%r2)
;   lrvr %r2, %r2
;   br %r14

function %atomic_rmw_umax_i8(i64, i64, i8) -> i8 {
block0(v0: i64, v1: i64, v2: i8):
  v3 = atomic_rmw.i8 little umax v1, v2
  return v3
}

; VCode:
; block0:
;   sllk %r2, %r3, 3
;   nill %r3, 65532
;   sllk %r4, %r4, 24
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   0: rll %r1, %r0, 0(%r2) ; clr %r4, %r1 ; jgnh 1f ; risbgn %r1, %r4, 32, 40, 0 ; rll %r1, %r1, 0(%r5) ; cs %r0, %r1, 0(%r3) ; jglh 0b ; 1:
;   rll %r2, %r0, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   sllk %r2, %r3, 3
;   nill %r3, 0xfffc
;   sllk %r4, %r4, 0x18
;   lcr %r5, %r2
;   l %r0, 0(%r3)
;   rll %r1, %r0, 0(%r2)
;   clr %r4, %r1
;   jgnh 0x3a
;   risbgn %r1, %r4, 0x20, 0x28, 0
;   rll %r1, %r1, 0(%r5)
;   cs %r0, %r1, 0(%r3)
;   jglh 0x16
;   rll %r2, %r0, 8(%r2)
;   br %r14

