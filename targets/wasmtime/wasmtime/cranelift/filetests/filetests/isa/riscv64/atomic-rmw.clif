test compile precise-output
set unwind_info=false
target riscv64

function %atomic_rmw_add_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 add v0, v1
    return
}

; VCode:
; block0:
;   amoadd.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoadd.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_add_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 add v0, v1
    return
}

; VCode:
; block0:
;   amoadd.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoadd.w.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_sub_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 sub v0, v1
    return
}

; VCode:
; block0:
;   sub a3,zero,a1
;   amoadd.d.aqrl a5,a3,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   neg a3, a1
;   amoadd.d.aqrl a5, a3, (a0)
;   ret

function %atomic_rmw_sub_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 sub v0, v1
    return
}

; VCode:
; block0:
;   sub a3,zero,a1
;   amoadd.w.aqrl a5,a3,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   neg a3, a1
;   amoadd.w.aqrl a5, a3, (a0)
;   ret

function %atomic_rmw_and_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 and v0, v1
    return
}

; VCode:
; block0:
;   amoand.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoand.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_and_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 and v0, v1
    return
}

; VCode:
; block0:
;   amoand.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoand.w.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_nand_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 nand v0, v1
    return
}

; VCode:
; block0:
;   atomic_rmw.i64 nand a3,a1,(a0)##t0=a4 offset=zero
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   lr.d.aqrl a3, (a0)
;   and a4, a1, a3
;   not a4, a4
;   sc.d.aqrl a4, a4, (a0)
;   bnez a4, -0x10
;   ret

function %atomic_rmw_nand_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 nand v0, v1
    return
}

; VCode:
; block0:
;   atomic_rmw.i32 nand a3,a1,(a0)##t0=a4 offset=zero
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   lr.w.aqrl a3, (a0)
;   and a4, a1, a3
;   not a4, a4
;   sc.w.aqrl a4, a4, (a0)
;   bnez a4, -0x10
;   ret

function %atomic_rmw_or_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 or v0, v1
    return
}

; VCode:
; block0:
;   amoor.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoor.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_or_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 or v0, v1
    return
}

; VCode:
; block0:
;   amoor.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoor.w.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_xor_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 xor v0, v1
    return
}

; VCode:
; block0:
;   amoxor.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoxor.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_xor_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 xor v0, v1
    return
}

; VCode:
; block0:
;   amoxor.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amoxor.w.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_smax_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 smax v0, v1
    return
}

; VCode:
; block0:
;   amomax.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amomax.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_smax_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 smax v0, v1
    return
}

; VCode:
; block0:
;   amomax.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amomax.w.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_umax_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 umax v0, v1
    return
}

; VCode:
; block0:
;   amomaxu.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amomaxu.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_umax_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 umax v0, v1
    return
}

; VCode:
; block0:
;   amomaxu.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amomaxu.w.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_smin_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 smin v0, v1
    return
}

; VCode:
; block0:
;   amomin.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amomin.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_smin_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 smin v0, v1
    return
}

; VCode:
; block0:
;   amomin.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amomin.w.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_umin_i64(i64, i64) {
block0(v0: i64, v1: i64):
    v2 = atomic_rmw.i64 umin v0, v1
    return
}

; VCode:
; block0:
;   amominu.d.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amominu.d.aqrl a3, a1, (a0)
;   ret

function %atomic_rmw_umin_i32(i64, i32) {
block0(v0: i64, v1: i32):
    v2 = atomic_rmw.i32 umin v0, v1
    return
}

; VCode:
; block0:
;   amominu.w.aqrl a3,a1,(a0)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   amominu.w.aqrl a3, a1, (a0)
;   ret

