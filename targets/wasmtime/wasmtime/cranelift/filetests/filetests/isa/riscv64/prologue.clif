test compile precise-output
set unwind_info=false
target riscv64

function %f(f64) -> f64 {
block0(v0: f64):
    v1 = fadd.f64 v0, v0
    v2 = fadd.f64 v0, v0
    v3 = fadd.f64 v0, v0
    v4 = fadd.f64 v0, v0
    v5 = fadd.f64 v0, v0
    v6 = fadd.f64 v0, v0
    v7 = fadd.f64 v0, v0
    v8 = fadd.f64 v0, v0
    v9 = fadd.f64 v0, v0
    v10 = fadd.f64 v0, v0
    v11 = fadd.f64 v0, v0
    v12 = fadd.f64 v0, v0
    v13 = fadd.f64 v0, v0
    v14 = fadd.f64 v0, v0
    v15 = fadd.f64 v0, v0
    v16 = fadd.f64 v0, v0
    v17 = fadd.f64 v0, v0
    v18 = fadd.f64 v0, v0
    v19 = fadd.f64 v0, v0
    v20 = fadd.f64 v0, v0
    v21 = fadd.f64 v0, v0
    v22 = fadd.f64 v0, v0
    v23 = fadd.f64 v0, v0
    v24 = fadd.f64 v0, v0
    v25 = fadd.f64 v0, v0
    v26 = fadd.f64 v0, v0
    v27 = fadd.f64 v0, v0
    v28 = fadd.f64 v0, v0
    v29 = fadd.f64 v0, v0
    v30 = fadd.f64 v0, v0
    v31 = fadd.f64 v0, v0

    v32 = fadd.f64 v0, v1
    v33 = fadd.f64 v2, v3
    v34 = fadd.f64 v4, v5
    v35 = fadd.f64 v6, v7
    v36 = fadd.f64 v8, v9
    v37 = fadd.f64 v10, v11
    v38 = fadd.f64 v12, v13
    v39 = fadd.f64 v14, v15
    v40 = fadd.f64 v16, v17
    v41 = fadd.f64 v18, v19
    v42 = fadd.f64 v20, v21
    v43 = fadd.f64 v22, v23
    v44 = fadd.f64 v24, v25
    v45 = fadd.f64 v26, v27
    v46 = fadd.f64 v28, v29
    v47 = fadd.f64 v30, v31

    v48 = fadd.f64 v32, v33
    v49 = fadd.f64 v34, v35
    v50 = fadd.f64 v36, v37
    v51 = fadd.f64 v38, v39
    v52 = fadd.f64 v40, v41
    v53 = fadd.f64 v42, v43
    v54 = fadd.f64 v44, v45
    v55 = fadd.f64 v46, v47

    v56 = fadd.f64 v48, v49
    v57 = fadd.f64 v50, v51
    v58 = fadd.f64 v52, v53
    v59 = fadd.f64 v54, v55

    v60 = fadd.f64 v56, v57
    v61 = fadd.f64 v58, v59

    v62 = fadd.f64 v60, v61

    return v62
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   fsd fs0,-8(sp)
;   fsd fs2,-16(sp)
;   fsd fs3,-24(sp)
;   fsd fs4,-32(sp)
;   fsd fs5,-40(sp)
;   fsd fs6,-48(sp)
;   fsd fs7,-56(sp)
;   fsd fs8,-64(sp)
;   fsd fs9,-72(sp)
;   fsd fs10,-80(sp)
;   fsd fs11,-88(sp)
;   addi sp,sp,-96
; block0:
;   fadd.d fa3,fa0,fa0,rne
;   fadd.d fa4,fa0,fa0,rne
;   fadd.d fa5,fa0,fa0,rne
;   fadd.d fa1,fa0,fa0,rne
;   fadd.d fa2,fa0,fa0,rne
;   fadd.d ft9,fa0,fa0,rne
;   fadd.d ft10,fa0,fa0,rne
;   fadd.d ft11,fa0,fa0,rne
;   fadd.d fs0,fa0,fa0,rne
;   fadd.d fs1,fa0,fa0,rne
;   fadd.d fs2,fa0,fa0,rne
;   fadd.d fs3,fa0,fa0,rne
;   fadd.d fs4,fa0,fa0,rne
;   fadd.d fs5,fa0,fa0,rne
;   fadd.d fs6,fa0,fa0,rne
;   fadd.d fs7,fa0,fa0,rne
;   fadd.d fs8,fa0,fa0,rne
;   fadd.d fs9,fa0,fa0,rne
;   fadd.d fs10,fa0,fa0,rne
;   fadd.d fs11,fa0,fa0,rne
;   fadd.d ft0,fa0,fa0,rne
;   fadd.d ft1,fa0,fa0,rne
;   fadd.d ft2,fa0,fa0,rne
;   fadd.d ft3,fa0,fa0,rne
;   fadd.d ft4,fa0,fa0,rne
;   fadd.d ft5,fa0,fa0,rne
;   fadd.d ft6,fa0,fa0,rne
;   fadd.d ft7,fa0,fa0,rne
;   fadd.d fa6,fa0,fa0,rne
;   fadd.d fa7,fa0,fa0,rne
;   fadd.d ft8,fa0,fa0,rne
;   fadd.d fa3,fa0,fa3,rne
;   fadd.d fa4,fa4,fa5,rne
;   fadd.d fa5,fa1,fa2,rne
;   fadd.d fa0,ft9,ft10,rne
;   fadd.d fa1,ft11,fs0,rne
;   fadd.d fa2,fs1,fs2,rne
;   fadd.d ft9,fs3,fs4,rne
;   fadd.d ft10,fs5,fs6,rne
;   fadd.d ft11,fs7,fs8,rne
;   fadd.d fs0,fs9,fs10,rne
;   fadd.d fs1,fs11,ft0,rne
;   fadd.d fs2,ft1,ft2,rne
;   fadd.d fs3,ft3,ft4,rne
;   fadd.d fs4,ft5,ft6,rne
;   fadd.d fs5,ft7,fa6,rne
;   fadd.d fs6,fa7,ft8,rne
;   fadd.d fa3,fa3,fa4,rne
;   fadd.d fa4,fa5,fa0,rne
;   fadd.d fa5,fa1,fa2,rne
;   fadd.d fa0,ft9,ft10,rne
;   fadd.d fa1,ft11,fs0,rne
;   fadd.d fa2,fs1,fs2,rne
;   fadd.d ft9,fs3,fs4,rne
;   fadd.d ft10,fs5,fs6,rne
;   fadd.d fa3,fa3,fa4,rne
;   fadd.d fa4,fa5,fa0,rne
;   fadd.d fa5,fa1,fa2,rne
;   fadd.d fa0,ft9,ft10,rne
;   fadd.d fa3,fa3,fa4,rne
;   fadd.d fa4,fa5,fa0,rne
;   fadd.d fa0,fa3,fa4,rne
;   addi sp,sp,96
;   fld fs0,-8(sp)
;   fld fs2,-16(sp)
;   fld fs3,-24(sp)
;   fld fs4,-32(sp)
;   fld fs5,-40(sp)
;   fld fs6,-48(sp)
;   fld fs7,-56(sp)
;   fld fs8,-64(sp)
;   fld fs9,-72(sp)
;   fld fs10,-80(sp)
;   fld fs11,-88(sp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   fsd fs0, -8(sp)
;   fsd fs2, -0x10(sp)
;   fsd fs3, -0x18(sp)
;   fsd fs4, -0x20(sp)
;   fsd fs5, -0x28(sp)
;   fsd fs6, -0x30(sp)
;   fsd fs7, -0x38(sp)
;   fsd fs8, -0x40(sp)
;   fsd fs9, -0x48(sp)
;   fsd fs10, -0x50(sp)
;   fsd fs11, -0x58(sp)
;   addi sp, sp, -0x60
; block1: ; offset 0x40
;   fadd.d fa3, fa0, fa0, rne
;   fadd.d fa4, fa0, fa0, rne
;   fadd.d fa5, fa0, fa0, rne
;   fadd.d fa1, fa0, fa0, rne
;   fadd.d fa2, fa0, fa0, rne
;   fadd.d ft9, fa0, fa0, rne
;   fadd.d ft10, fa0, fa0, rne
;   fadd.d ft11, fa0, fa0, rne
;   fadd.d fs0, fa0, fa0, rne
;   fadd.d fs1, fa0, fa0, rne
;   fadd.d fs2, fa0, fa0, rne
;   fadd.d fs3, fa0, fa0, rne
;   fadd.d fs4, fa0, fa0, rne
;   fadd.d fs5, fa0, fa0, rne
;   fadd.d fs6, fa0, fa0, rne
;   fadd.d fs7, fa0, fa0, rne
;   fadd.d fs8, fa0, fa0, rne
;   fadd.d fs9, fa0, fa0, rne
;   fadd.d fs10, fa0, fa0, rne
;   fadd.d fs11, fa0, fa0, rne
;   fadd.d ft0, fa0, fa0, rne
;   fadd.d ft1, fa0, fa0, rne
;   fadd.d ft2, fa0, fa0, rne
;   fadd.d ft3, fa0, fa0, rne
;   fadd.d ft4, fa0, fa0, rne
;   fadd.d ft5, fa0, fa0, rne
;   fadd.d ft6, fa0, fa0, rne
;   fadd.d ft7, fa0, fa0, rne
;   fadd.d fa6, fa0, fa0, rne
;   fadd.d fa7, fa0, fa0, rne
;   fadd.d ft8, fa0, fa0, rne
;   fadd.d fa3, fa0, fa3, rne
;   fadd.d fa4, fa4, fa5, rne
;   fadd.d fa5, fa1, fa2, rne
;   fadd.d fa0, ft9, ft10, rne
;   fadd.d fa1, ft11, fs0, rne
;   fadd.d fa2, fs1, fs2, rne
;   fadd.d ft9, fs3, fs4, rne
;   fadd.d ft10, fs5, fs6, rne
;   fadd.d ft11, fs7, fs8, rne
;   fadd.d fs0, fs9, fs10, rne
;   fadd.d fs1, fs11, ft0, rne
;   fadd.d fs2, ft1, ft2, rne
;   fadd.d fs3, ft3, ft4, rne
;   fadd.d fs4, ft5, ft6, rne
;   fadd.d fs5, ft7, fa6, rne
;   fadd.d fs6, fa7, ft8, rne
;   fadd.d fa3, fa3, fa4, rne
;   fadd.d fa4, fa5, fa0, rne
;   fadd.d fa5, fa1, fa2, rne
;   fadd.d fa0, ft9, ft10, rne
;   fadd.d fa1, ft11, fs0, rne
;   fadd.d fa2, fs1, fs2, rne
;   fadd.d ft9, fs3, fs4, rne
;   fadd.d ft10, fs5, fs6, rne
;   fadd.d fa3, fa3, fa4, rne
;   fadd.d fa4, fa5, fa0, rne
;   fadd.d fa5, fa1, fa2, rne
;   fadd.d fa0, ft9, ft10, rne
;   fadd.d fa3, fa3, fa4, rne
;   fadd.d fa4, fa5, fa0, rne
;   fadd.d fa0, fa3, fa4, rne
;   addi sp, sp, 0x60
;   fld fs0, -8(sp)
;   fld fs2, -0x10(sp)
;   fld fs3, -0x18(sp)
;   fld fs4, -0x20(sp)
;   fld fs5, -0x28(sp)
;   fld fs6, -0x30(sp)
;   fld fs7, -0x38(sp)
;   fld fs8, -0x40(sp)
;   fld fs9, -0x48(sp)
;   fld fs10, -0x50(sp)
;   fld fs11, -0x58(sp)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %f2(i64) -> i64 {
block0(v0: i64):
    v1 = iadd.i64 v0, v0
    v2 = iadd.i64 v0, v1
    v3 = iadd.i64 v0, v2
    v4 = iadd.i64 v0, v3
    v5 = iadd.i64 v0, v4
    v6 = iadd.i64 v0, v5
    v7 = iadd.i64 v0, v6
    v8 = iadd.i64 v0, v7
    v9 = iadd.i64 v0, v8
    v10 = iadd.i64 v0, v9
    v11 = iadd.i64 v0, v10
    v12 = iadd.i64 v0, v11
    v13 = iadd.i64 v0, v12
    v14 = iadd.i64 v0, v13
    v15 = iadd.i64 v0, v14
    v16 = iadd.i64 v0, v15
    v17 = iadd.i64 v0, v16
    v18 = iadd.i64 v0, v17

    v19 = iadd.i64 v0, v1
    v20 = iadd.i64 v2, v3
    v21 = iadd.i64 v4, v5
    v22 = iadd.i64 v6, v7
    v23 = iadd.i64 v8, v9
    v24 = iadd.i64 v10, v11
    v25 = iadd.i64 v12, v13
    v26 = iadd.i64 v14, v15
    v27 = iadd.i64 v16, v17

    v28 = iadd.i64 v18, v19
    v29 = iadd.i64 v20, v21
    v30 = iadd.i64 v22, v23
    v31 = iadd.i64 v24, v25
    v32 = iadd.i64 v26, v27

    v33 = iadd.i64 v28, v29
    v34 = iadd.i64 v30, v31

    v35 = iadd.i64 v32, v33
    v36 = iadd.i64 v34, v35

    return v36
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   sd s1,-8(sp)
;   sd s2,-16(sp)
;   sd s3,-24(sp)
;   sd s4,-32(sp)
;   sd s5,-40(sp)
;   sd s6,-48(sp)
;   sd s7,-56(sp)
;   addi sp,sp,-64
; block0:
;   add a1,a0,a0
;   add a2,a0,a1
;   add a3,a0,a2
;   add a4,a0,a3
;   add a5,a0,a4
;   add t1,a0,a5
;   add t2,a0,t1
;   add a6,a0,t2
;   add a7,a0,a6
;   add t3,a0,a7
;   add t4,a0,t3
;   add s1,a0,t4
;   add s2,a0,s1
;   add s3,a0,s2
;   add s4,a0,s3
;   add s5,a0,s4
;   add s6,a0,s5
;   add s7,a0,s6
;   add a1,a0,a1
;   add a2,a2,a3
;   add a3,a4,a5
;   add a4,t1,t2
;   add a5,a6,a7
;   add a0,t3,t4
;   add t1,s1,s2
;   add t2,s3,s4
;   add a6,s5,s6
;   add a1,s7,a1
;   add a2,a2,a3
;   add a3,a4,a5
;   add a4,a0,t1
;   add a5,t2,a6
;   add a1,a1,a2
;   add a2,a3,a4
;   add a1,a5,a1
;   add a0,a2,a1
;   addi sp,sp,64
;   ld s1,-8(sp)
;   ld s2,-16(sp)
;   ld s3,-24(sp)
;   ld s4,-32(sp)
;   ld s5,-40(sp)
;   ld s6,-48(sp)
;   ld s7,-56(sp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   sd s1, -8(sp)
;   sd s2, -0x10(sp)
;   sd s3, -0x18(sp)
;   sd s4, -0x20(sp)
;   sd s5, -0x28(sp)
;   sd s6, -0x30(sp)
;   sd s7, -0x38(sp)
;   addi sp, sp, -0x40
; block1: ; offset 0x30
;   add a1, a0, a0
;   add a2, a0, a1
;   add a3, a0, a2
;   add a4, a0, a3
;   add a5, a0, a4
;   add t1, a0, a5
;   add t2, a0, t1
;   add a6, a0, t2
;   add a7, a0, a6
;   add t3, a0, a7
;   add t4, a0, t3
;   add s1, a0, t4
;   add s2, a0, s1
;   add s3, a0, s2
;   add s4, a0, s3
;   add s5, a0, s4
;   add s6, a0, s5
;   add s7, a0, s6
;   add a1, a0, a1
;   add a2, a2, a3
;   add a3, a4, a5
;   add a4, t1, t2
;   add a5, a6, a7
;   add a0, t3, t4
;   add t1, s1, s2
;   add t2, s3, s4
;   add a6, s5, s6
;   add a1, s7, a1
;   add a2, a2, a3
;   add a3, a4, a5
;   add a4, a0, t1
;   add a5, t2, a6
;   add a1, a1, a2
;   add a2, a3, a4
;   add a1, a5, a1
;   add a0, a2, a1
;   addi sp, sp, 0x40
;   ld s1, -8(sp)
;   ld s2, -0x10(sp)
;   ld s3, -0x18(sp)
;   ld s4, -0x20(sp)
;   ld s5, -0x28(sp)
;   ld s6, -0x30(sp)
;   ld s7, -0x38(sp)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

