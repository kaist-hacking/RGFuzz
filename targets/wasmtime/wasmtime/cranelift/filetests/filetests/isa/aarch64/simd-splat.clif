test compile precise-output
set unwind_info=false
target aarch64

function %splat_load(i64) -> i64x2 {
block0(v0: i64):
    v1 = load.i64 v0
    v2 = splat.i64x2 v1
    return v2
}

; VCode:
; block0:
;   ld1r { v0.2d }, [x0]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ld1r {v0.2d}, [x0]
;   ret

function %splat_load2(i64) -> i64x2 {
block0(v0: i64):
    v1 = load.i64 v0+100
    v2 = splat.i64x2 v1
    return v2
}

; VCode:
; block0:
;   add x2, x0, #100
;   ld1r { v0.2d }, [x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x2, x0, #0x64
;   ld1r {v0.2d}, [x2]
;   ret

function %splat_load3(i64) -> i64x2 {
block0(v0: i64):
    v1 = load.i64 v0+0xfff0000
    v2 = splat.i64x2 v1
    return v2
}

; VCode:
; block0:
;   movz x2, #4095, LSL #16
;   add x4, x0, x2
;   ld1r { v0.2d }, [x4]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x2, #0xfff0000
;   add x4, x0, x2
;   ld1r {v0.2d}, [x4]
;   ret

function %splat_load4(i64, i64) -> i64x2 {
block0(v0: i64, v1: i64):
    v2 = iadd v0, v1
    v3 = load.i64 v2
    v4 = splat.i64x2 v3
    return v4
}

; VCode:
; block0:
;   add x4, x0, x1
;   ld1r { v0.2d }, [x4]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x4, x0, x1
;   ld1r {v0.2d}, [x4]
;   ret

function %splat_load5(i64, i64) -> i64x2 {
block0(v0: i64, v1: i64):
    v2 = iadd v0, v1
    v3 = load.i64 v2+100
    v4 = splat.i64x2 v3
    return v4
}

; VCode:
; block0:
;   add x5, x0, x1
;   add x4, x5, #100
;   ld1r { v0.2d }, [x4]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x5, x0, x1
;   add x4, x5, #0x64
;   ld1r {v0.2d}, [x4]
;   ret

function %splat_load6(i64, i64) -> i64x2 {
block0(v0: i64, v1: i64):
    v2 = imul_imm v1, 2
    v3 = iadd v0, v2
    v4 = load.i64 v3+100
    v5 = splat.i64x2 v4
    return v5
}

; VCode:
; block0:
;   movz x6, #2
;   madd x6, x1, x6, x0
;   add x5, x6, #100
;   ld1r { v0.2d }, [x5]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x6, #2
;   madd x6, x1, x6, x0
;   add x5, x6, #0x64
;   ld1r {v0.2d}, [x5]
;   ret

