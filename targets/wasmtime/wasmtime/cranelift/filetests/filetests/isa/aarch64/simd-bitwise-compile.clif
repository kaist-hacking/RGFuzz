test compile precise-output
target aarch64

function %band_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = band v0, v1
    return v2
}

; VCode:
; block0:
;   and v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   and v0.16b, v0.16b, v1.16b
;   ret

function %band_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
    v2 = band v0, v1
    return v2
}

; VCode:
; block0:
;   and v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   and v0.16b, v0.16b, v1.16b
;   ret

function %band_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = band v0, v1
    return v2
}

; VCode:
; block0:
;   and v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   and v0.16b, v0.16b, v1.16b
;   ret

function %bor_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = bor v0, v1
    return v2
}

; VCode:
; block0:
;   orr v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   orr v0.16b, v0.16b, v1.16b
;   ret

function %bor_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
    v2 = bor v0, v1
    return v2
}

; VCode:
; block0:
;   orr v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   orr v0.16b, v0.16b, v1.16b
;   ret

function %bor_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bor v0, v1
    return v2
}

; VCode:
; block0:
;   orr v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   orr v0.16b, v0.16b, v1.16b
;   ret

function %bxor_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = bxor v0, v1
    return v2
}

; VCode:
; block0:
;   eor v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   eor v0.16b, v0.16b, v1.16b
;   ret

function %bxor_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
    v2 = bxor v0, v1
    return v2
}

; VCode:
; block0:
;   eor v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   eor v0.16b, v0.16b, v1.16b
;   ret

function %bxor_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bxor v0, v1
    return v2
}

; VCode:
; block0:
;   eor v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   eor v0.16b, v0.16b, v1.16b
;   ret

function %bitselect_i16x8() -> i16x8 {
block0:
    v0 = vconst.i16x8 [0 0 0 0 0 0 0 0]
    v1 = vconst.i16x8 [0 0 0 0 0 0 0 0]
    v2 = vconst.i16x8 [0 0 0 0 0 0 0 0]
    v3 = bitselect v0, v1, v2
    return v3
}

; VCode:
; block0:
;   movi v0.16b, #0
;   movi v3.16b, #0
;   movi v4.16b, #0
;   bsl v0.16b, v0.16b, v3.16b, v4.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   movi v0.16b, #0
;   movi v3.16b, #0
;   movi v4.16b, #0
;   bsl v0.16b, v3.16b, v4.16b
;   ret

function %vselect_i16x8(i16x8, i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8, v2: i16x8):
    v3 = bitselect v0, v1, v2
    return v3
}

; VCode:
; block0:
;   bsl v0.16b, v0.16b, v1.16b, v2.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   bsl v0.16b, v1.16b, v2.16b
;   ret

function %vselect_f32x4(f32x4, f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4, v2: f32x4):
    v3 = bitselect v0, v1, v2
    return v3
}

; VCode:
; block0:
;   bsl v0.16b, v0.16b, v1.16b, v2.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   bsl v0.16b, v1.16b, v2.16b
;   ret

function %vselect_f64x2(f64x2, f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2, v2: f64x2):
    v3 = bitselect v0, v1, v2
    return v3
}

; VCode:
; block0:
;   bsl v0.16b, v0.16b, v1.16b, v2.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   bsl v0.16b, v1.16b, v2.16b
;   ret

function %ishl_i8x16(i32) -> i8x16 {
block0(v0: i32):
    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v2 = ishl v1, v0
    return v2
}

; VCode:
; block0:
;   ldr q5, [const(0)]
;   and w3, w0, #7
;   dup v6.16b, w3
;   sshl v0.16b, v5.16b, v6.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr q5, #0x20
;   and w3, w0, #7
;   dup v6.16b, w3
;   sshl v0.16b, v5.16b, v6.16b
;   ret
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x01, 0x02, 0x03
;   .byte 0x04, 0x05, 0x06, 0x07
;   add w8, w8, w10, lsl #2
;   .byte 0x0c, 0x0d, 0x0e, 0x0f

function %ushr_i8x16_imm() -> i8x16 {
block0:
    v0 = iconst.i32 1
    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v2 = ushr v1, v0
    return v2
}

; VCode:
; block0:
;   ldr q1, [const(0)]
;   ushr v0.16b, v1.16b, #1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr q1, #0x10
;   ushr v0.16b, v1.16b, #1
;   ret
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x01, 0x02, 0x03
;   .byte 0x04, 0x05, 0x06, 0x07
;   add w8, w8, w10, lsl #2
;   .byte 0x0c, 0x0d, 0x0e, 0x0f

function %sshr_i8x16(i32) -> i8x16 {
block0(v0: i32):
    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v2 = sshr v1, v0
    return v2
}

; VCode:
; block0:
;   ldr q6, [const(0)]
;   and w3, w0, #7
;   sub x5, xzr, x3
;   dup v7.16b, w5
;   sshl v0.16b, v6.16b, v7.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr q6, #0x20
;   and w3, w0, #7
;   neg x5, x3
;   dup v7.16b, w5
;   sshl v0.16b, v6.16b, v7.16b
;   ret
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x01, 0x02, 0x03
;   .byte 0x04, 0x05, 0x06, 0x07
;   add w8, w8, w10, lsl #2
;   .byte 0x0c, 0x0d, 0x0e, 0x0f

function %sshr_i8x16_imm(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
    v2 = sshr_imm v0, 3
    return v2
}

; VCode:
; block0:
;   sshr v0.16b, v0.16b, #3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sshr v0.16b, v0.16b, #3
;   ret

function %sshr_i64x2(i64x2, i32) -> i64x2 {
block0(v0: i64x2, v1: i32):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   and w3, w0, #63
;   sub x5, xzr, x3
;   dup v7.2d, x5
;   sshl v0.2d, v0.2d, v7.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   and w3, w0, #0x3f
;   neg x5, x3
;   dup v7.2d, x5
;   sshl v0.2d, v0.2d, v7.2d
;   ret

